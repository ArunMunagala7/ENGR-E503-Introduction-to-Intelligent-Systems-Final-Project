{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunMunagala7/ENGR-E503-Introduction-to-Intelligent-Systems-Final-Project/blob/main/ISE_Project_Final_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXv4ZE_QsJN-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Q-Learning Integrated EKF-SLAM for Unicycle Robot Trajectory Control\n",
        "\n",
        "This script implements an Extended Kalman Filter (EKF) for Simultaneous Localization and Mapping (SLAM)\n",
        "combined with Q-Learning for adaptive trajectory control on a racetrack. The robot uses a unicycle model,\n",
        "range-bearing sensor, and learns to adjust angular velocity based on lateral and heading errors.\n",
        "\n",
        "Author: Generated from Q_Learning.ipynb\n",
        "Date: December 15, 2025\n",
        "\"\"\"\n",
        "\n",
        "# Install necessary packages for headless environment (e.g., in Colab or server)\n",
        "!apt-get install -y ffmpeg xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "# Set up virtual display for headless plotting\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(800, 600))\n",
        "display.start()\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np  # Numerical computations\n",
        "import matplotlib.pyplot as plt  # Plotting\n",
        "from matplotlib.patches import Ellipse  # For covariance ellipses\n",
        "from matplotlib.animation import FuncAnimation  # For animations\n",
        "from IPython.display import HTML  # For displaying animations in notebooks\n",
        "\n",
        "class EKFSLAM:\n",
        "    \"\"\"\n",
        "    Extended Kalman Filter for SLAM with unicycle robot model.\n",
        "    Maintains joint state estimate of robot pose and landmark positions.\n",
        "    \"\"\"\n",
        "    def __init__(self, robot_sigma_x=0.01, robot_sigma_y=0.01, robot_sigma_theta=0.01,\n",
        "                 measurement_sigma_r=0.1, measurement_sigma_b=0.05,\n",
        "                 gating_threshold=9.21):\n",
        "        \"\"\"\n",
        "        Initialize EKF-SLAM.\n",
        "\n",
        "        Args:\n",
        "            robot_sigma_x, robot_sigma_y, robot_sigma_theta: Initial pose uncertainties\n",
        "            measurement_sigma_r, measurement_sigma_b: Measurement noise std devs\n",
        "            gating_threshold: Mahalanobis distance threshold for data association\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        The state vector mu contains robot pose [x, y, theta] followed by landmark positions [mx1, my1, mx2, my2, ...].\n",
        "        Sigma is the covariance matrix for the entire state.\n",
        "        Q is the measurement noise covariance for range and bearing.\n",
        "        lm_size tracks the number of landmarks in the map.\n",
        "        \"\"\"\n",
        "        self.mu = np.array([0.0, 0.0, 0.0], dtype=float)  # State mean: [x, y, theta]\n",
        "        self.Sigma = np.diag([robot_sigma_x, robot_sigma_y, robot_sigma_theta])  # Initial covariance\n",
        "        self.Q = np.diag([measurement_sigma_r**2, measurement_sigma_b**2])  # Measurement noise covariance\n",
        "        self.lm_size = 0  # Number of landmarks in map\n",
        "        self.gating = gating_threshold  # Gating threshold for association\n",
        "\n",
        "    def prediction(self, v, omega, dt, sigma_v=0.1, sigma_omega=0.1):\n",
        "        \"\"\"\n",
        "        Prediction step: Update state and covariance based on motion model.\n",
        "\n",
        "        Args:\n",
        "            v: Linear velocity\n",
        "            omega: Angular velocity\n",
        "            dt: Time step\n",
        "            sigma_v, sigma_omega: Process noise std devs\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        The unicycle motion model updates the robot's position and orientation.\n",
        "        For small omega, it's approximately straight line; otherwise, circular arc.\n",
        "        The Jacobian G linearizes the motion for covariance propagation.\n",
        "        \"\"\"\n",
        "        theta = self.mu[2]\n",
        "        if abs(omega) < 1e-6:  # Straight line motion\n",
        "            dx = v * dt * np.cos(theta)\n",
        "            dy = v * dt * np.sin(theta)\n",
        "            dtheta = omega * dt\n",
        "            G = np.eye(3)  # Jacobian matrix\n",
        "            G[0, 2] = -v * dt * np.sin(theta)\n",
        "            G[1, 2] = v * dt * np.cos(theta)\n",
        "        else:  # Circular motion\n",
        "            dx = (v / omega) * (np.sin(theta + omega * dt) - np.sin(theta))\n",
        "            dy = (v / omega) * (-np.cos(theta + omega * dt) + np.cos(theta))\n",
        "            dtheta = omega * dt\n",
        "            G = np.eye(3)\n",
        "            G[0, 2] = (v / omega) * (np.cos(theta + omega * dt) - np.cos(theta))\n",
        "            G[1, 2] = (v / omega) * (np.sin(theta + omega * dt) - np.sin(theta))\n",
        "\n",
        "        # Update state mean\n",
        "        self.mu[0] += dx\n",
        "        self.mu[1] += dy\n",
        "        self.mu[2] += dtheta\n",
        "        self.mu[2] = (self.mu[2] + np.pi) % (2 * np.pi) - np.pi  # Normalize angle\n",
        "\n",
        "        # Update covariance\n",
        "        n = len(self.mu)\n",
        "        full_G = np.eye(n)\n",
        "        full_G[0:3, 0:3] = G\n",
        "        R_robot = np.diag([(sigma_v * dt)**2, (sigma_v * dt)**2, (sigma_omega * dt)**2])\n",
        "        full_R = np.zeros((n, n))\n",
        "        full_R[0:3, 0:3] = R_robot\n",
        "        self.Sigma = full_G @ self.Sigma @ full_G.T + full_R\n",
        "\n",
        "    def update(self, zs):\n",
        "        \"\"\"\n",
        "        Update step: Incorporate measurements using data association.\n",
        "\n",
        "        Args:\n",
        "            zs: List of measurements [(r, b), ...]\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        For each measurement, find the best matching landmark using Mahalanobis distance.\n",
        "        If no good match, initialize a new landmark. Otherwise, update the existing one.\n",
        "        This implements the core of SLAM: associating observations to map features.\n",
        "        \"\"\"\n",
        "        for z in zs:\n",
        "            r, b = z\n",
        "            min_d = float('inf')\n",
        "            best_j = -1\n",
        "            best_H = None\n",
        "            best_S = None\n",
        "            best_innov = None\n",
        "\n",
        "            # Find best matching landmark\n",
        "            for lm in range(self.lm_size):\n",
        "                j = 3 + 2 * lm  # Landmark index in state vector\n",
        "                mx = self.mu[j]\n",
        "                my = self.mu[j + 1]\n",
        "                dx = mx - self.mu[0]\n",
        "                dy = my - self.mu[1]\n",
        "                q = dx**2 + dy**2\n",
        "                sqrtq = np.sqrt(q)\n",
        "                z_hat = np.array([sqrtq, np.arctan2(dy, dx) - self.mu[2]])  # Predicted measurement\n",
        "                z_hat[1] = (z_hat[1] + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "                # Jacobian H\n",
        "                H = np.zeros((2, len(self.mu)))\n",
        "                H[0, 0] = -dx / sqrtq\n",
        "                H[0, 1] = -dy / sqrtq\n",
        "                H[0, 2] = 0\n",
        "                H[1, 0] = dy / q\n",
        "                H[1, 1] = -dx / q\n",
        "                H[1, 2] = -1\n",
        "                H[0, j] = dx / sqrtq\n",
        "                H[0, j + 1] = dy / sqrtq\n",
        "                H[1, j] = -dy / q\n",
        "                H[1, j + 1] = dx / q\n",
        "\n",
        "                S = H @ self.Sigma @ H.T + self.Q  # Innovation covariance\n",
        "                innov = np.array([r - z_hat[0], b - z_hat[1]])\n",
        "                innov[1] = (innov[1] + np.pi) % (2 * np.pi) - np.pi\n",
        "                d = innov.T @ np.linalg.inv(S) @ innov  # Mahalanobis distance\n",
        "\n",
        "                if d < min_d:\n",
        "                    min_d = d\n",
        "                    best_j = j\n",
        "                    best_H = H\n",
        "                    best_S = S\n",
        "                    best_innov = innov\n",
        "\n",
        "            \"\"\"\n",
        "            After checking all landmarks, decide whether to initialize a new landmark\n",
        "            or update an existing one based on the gating threshold.\n",
        "            This handles the exploration aspect of SLAM.\n",
        "            \"\"\"\n",
        "            if best_j == -1 or min_d > self.gating:  # No match or poor match: initialize new landmark\n",
        "                theta = self.mu[2]\n",
        "                cos_tb = np.cos(theta + b)\n",
        "                sin_tb = np.sin(theta + b)\n",
        "                mx = self.mu[0] + r * cos_tb\n",
        "                my = self.mu[1] + r * sin_tb\n",
        "                G_pose = np.array([[1, 0, -r * sin_tb],\n",
        "                                   [0, 1, r * cos_tb]])\n",
        "                G_z = np.array([[cos_tb, -r * sin_tb],\n",
        "                                [sin_tb, r * cos_tb]])\n",
        "                self.mu = np.append(self.mu, [mx, my])\n",
        "                n = len(self.mu)\n",
        "                old_n = n - 2\n",
        "                new_Sigma = np.zeros((n, n))\n",
        "                new_Sigma[0:old_n, 0:old_n] = self.Sigma\n",
        "                lm_cov = G_pose @ self.Sigma[0:3, 0:3] @ G_pose.T + G_z @ self.Q @ G_z.T\n",
        "                new_Sigma[old_n:, old_n:] = lm_cov\n",
        "                cross = self.Sigma[0:old_n, 0:3] @ G_pose.T\n",
        "                new_Sigma[0:old_n, old_n:] = cross\n",
        "                new_Sigma[old_n:, 0:old_n] = cross.T\n",
        "                self.Sigma = new_Sigma\n",
        "                self.lm_size += 1\n",
        "            else:  # Update existing landmark\n",
        "                K = self.Sigma @ best_H.T @ np.linalg.inv(best_S)  # Kalman gain\n",
        "                self.mu += K @ best_innov\n",
        "                self.mu[2] = (self.mu[2] + np.pi) % (2 * np.pi) - np.pi\n",
        "                I = np.eye(len(self.mu))\n",
        "                self.Sigma = (I - K @ best_H) @ self.Sigma  # Joseph form update\n",
        "\n",
        "def plot_cov_ellipse(ax, pos, cov, color='blue', alpha=0.3):\n",
        "    \"\"\"\n",
        "    Plot covariance ellipse on given axes.\n",
        "\n",
        "    Args:\n",
        "        ax: Matplotlib axes\n",
        "        pos: Position [x, y]\n",
        "        cov: 2x2 covariance matrix\n",
        "        color: Ellipse color\n",
        "        alpha: Transparency\n",
        "    \"\"\"\n",
        "    if cov.shape == (2, 2):\n",
        "        lambda_, v = np.linalg.eig(cov)\n",
        "        lambda_ = np.sqrt(np.abs(lambda_))\n",
        "        angle = np.rad2deg(np.arccos(v[0, 0]))\n",
        "        width, height = 2 * lambda_[0], 2 * lambda_[1]\n",
        "        ell = Ellipse(xy=pos, width=width, height=height, angle=angle)\n",
        "        ell.set_facecolor(color)\n",
        "        ell.set_alpha(alpha)\n",
        "        ax.add_artist(ell)\n",
        "\n",
        "class TargetCourse:\n",
        "    \"\"\"\n",
        "    Represents the reference trajectory (racetrack path).\n",
        "    Provides methods to find nearest point on path.\n",
        "    \"\"\"\n",
        "    def __init__(self, cx, cy):\n",
        "        \"\"\"\n",
        "        Initialize with path coordinates.\n",
        "\n",
        "        Args:\n",
        "            cx, cy: Arrays of x and y coordinates of the path\n",
        "        \"\"\"\n",
        "        self.cx = cx\n",
        "        self.cy = cy\n",
        "        self.old_nearest_point_index = None\n",
        "\n",
        "    def search_target_index(self, x, y, yaw):\n",
        "        \"\"\"\n",
        "        Find the target point on the path for pure pursuit.\n",
        "\n",
        "        Args:\n",
        "            x, y, yaw: Robot position and orientation\n",
        "\n",
        "        Returns:\n",
        "            ind: Index of target point\n",
        "            Lf: Look-ahead distance\n",
        "        \"\"\"\n",
        "        if self.old_nearest_point_index is None:\n",
        "            dx = [x - icx for icx in self.cx]\n",
        "            dy = [y - icy for icy in self.cy]\n",
        "            d = np.hypot(dx, dy)\n",
        "            ind = np.argmin(d)\n",
        "            self.old_nearest_point_index = ind\n",
        "        else:\n",
        "            ind = self.old_nearest_point_index\n",
        "            distance_this_index = np.hypot(x - self.cx[ind], y - self.cy[ind])\n",
        "            while True:\n",
        "                next_ind = (ind + 1) % len(self.cx)\n",
        "                distance_next_index = np.hypot(x - self.cx[next_ind], y - self.cy[next_ind])\n",
        "                if distance_this_index < distance_next_index:\n",
        "                    break\n",
        "                ind = next_ind\n",
        "                distance_this_index = distance_next_index\n",
        "            self.old_nearest_point_index = ind\n",
        "\n",
        "        Lf = 0.1 * 1.0 + 2.0  # Look-ahead distance: k * v + Lfc\n",
        "\n",
        "        while Lf > np.hypot(x - self.cx[ind], y - self.cy[ind]):\n",
        "            ind = (ind + 1) % len(self.cx)\n",
        "        return ind, Lf\n",
        "\n",
        "def pure_pursuit(x, y, yaw, course, pind):\n",
        "    \"\"\"\n",
        "    Pure pursuit algorithm for steering control.\n",
        "\n",
        "    Args:\n",
        "        x, y, yaw: Robot pose\n",
        "        course: TargetCourse object\n",
        "        pind: Previous target index\n",
        "\n",
        "    Returns:\n",
        "        delta: Steering angle\n",
        "        ind: New target index\n",
        "    \"\"\"\n",
        "    ind, Lf = course.search_target_index(x, y, yaw)\n",
        "    if pind >= ind:\n",
        "        ind = pind\n",
        "    if ind < len(course.cx):\n",
        "        tx = course.cx[ind]\n",
        "        ty = course.cy[ind]\n",
        "    else:\n",
        "        tx = course.cx[-1]\n",
        "        ty = course.cy[-1]\n",
        "        ind = len(course.cx) - 1\n",
        "    alpha = np.arctan2(ty - y, tx - x) - yaw\n",
        "    WB = 0.5  # Wheelbase\n",
        "    delta = np.arctan2(2.0 * WB * np.sin(alpha) / Lf, 1.0)\n",
        "    return delta, ind\n",
        "\n",
        "class QLearningController:\n",
        "    \"\"\"\n",
        "    Q-Learning controller for adjusting angular velocity based on errors.\n",
        "    Learns optimal ω offsets to minimize lateral and heading errors.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_bins=10, num_actions=5, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Q-Learning controller.\n",
        "\n",
        "        Args:\n",
        "            num_bins: Number of bins for state discretization\n",
        "            num_actions: Number of possible actions (ω offsets)\n",
        "            alpha: Learning rate\n",
        "            gamma: Discount factor\n",
        "            epsilon: Exploration rate\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        The Q-table is 3D: [lat_bins, head_bins, actions].\n",
        "        States are discretized lateral and heading errors.\n",
        "        Actions are offsets to angular velocity for fine-tuning control.\n",
        "        \"\"\"\n",
        "        self.num_bins = num_bins\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = np.linspace(-0.2, 0.2, num_actions)  # ω offsets\n",
        "        self.q_table = np.zeros((num_bins, num_bins, num_actions))  # Q-table\n",
        "        self.lat_bins = np.linspace(-2, 2, num_bins)  # Lateral error bins\n",
        "        self.head_bins = np.linspace(-np.pi/2, np.pi/2, num_bins)  # Heading error bins\n",
        "\n",
        "    def discretize(self, lat_err, head_err):\n",
        "        \"\"\"\n",
        "        Discretize continuous errors into state indices.\n",
        "\n",
        "        Args:\n",
        "            lat_err: Lateral error\n",
        "            head_err: Heading error\n",
        "\n",
        "        Returns:\n",
        "            lat_idx, head_idx: Discretized state\n",
        "        \"\"\"\n",
        "        lat_idx = np.digitize(lat_err, self.lat_bins) - 1\n",
        "        head_idx = np.digitize(head_err, self.head_bins) - 1\n",
        "        lat_idx = np.clip(lat_idx, 0, self.num_bins - 1)\n",
        "        head_idx = np.clip(head_idx, 0, self.num_bins - 1)\n",
        "        return lat_idx, head_idx\n",
        "\n",
        "    def choose_action(self, state, train=True):\n",
        "        \"\"\"\n",
        "        Choose action using ε-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state: (lat_idx, head_idx)\n",
        "            train: Whether in training mode\n",
        "\n",
        "        Returns:\n",
        "            action_idx: Index of chosen action\n",
        "        \"\"\"\n",
        "        if train and np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Update Q-table using temporal difference learning.\n",
        "\n",
        "        Args:\n",
        "            state: Current state\n",
        "            action: Action taken\n",
        "            reward: Reward received\n",
        "            next_state: Next state\n",
        "        \"\"\"\n",
        "        q_predict = self.q_table[state][action]\n",
        "        q_target = reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_predict)\n",
        "\n",
        "def generate_racetrack(L=20, W=10, num_straight=50, num_curve=50):\n",
        "    \"\"\"\n",
        "    Generate oval racetrack coordinates.\n",
        "\n",
        "    Args:\n",
        "        L: Length of straight sections\n",
        "        W: Width (diameter of semicircles)\n",
        "        num_straight, num_curve: Number of points per section\n",
        "\n",
        "    Returns:\n",
        "        cx, cy: Arrays of x and y coordinates\n",
        "    \"\"\"\n",
        "    r = W / 2\n",
        "    # Bottom straight\n",
        "    x_bottom = np.linspace(0, L, num_straight)\n",
        "    y_bottom = np.zeros(num_straight)\n",
        "    # Right semicircle\n",
        "    theta_right = np.linspace(-np.pi/2, np.pi/2, num_curve)\n",
        "    x_right = L + r * np.cos(theta_right)\n",
        "    y_right = r + r * np.sin(theta_right)\n",
        "    # Top straight\n",
        "    x_top = np.linspace(L, 0, num_straight)\n",
        "    y_top = np.ones(num_straight) * W\n",
        "    # Left semicircle\n",
        "    theta_left = np.linspace(np.pi/2, np.pi/2 + np.pi, num_curve)\n",
        "    x_left = 0 + r * np.cos(theta_left)\n",
        "    y_left = r + r * np.sin(theta_left)\n",
        "    cx = np.concatenate([x_bottom[:-1], x_right[:-1], x_top[:-1], x_left])\n",
        "    cy = np.concatenate([y_bottom[:-1], y_right[:-1], y_top[:-1], y_left])\n",
        "    return cx, cy\n",
        "\n",
        "def get_errors(x, y, yaw, course):\n",
        "    \"\"\"\n",
        "    Compute lateral and heading errors relative to path.\n",
        "\n",
        "    Args:\n",
        "        x, y, yaw: Robot pose\n",
        "        course: TargetCourse object\n",
        "\n",
        "    Returns:\n",
        "        lat_err: Signed lateral error\n",
        "        head_err: Heading error\n",
        "    \"\"\"\n",
        "    ind, _ = course.search_target_index(x, y, yaw)\n",
        "    tx, ty = course.cx[ind], course.cy[ind]\n",
        "    head_err = np.arctan2(ty - y, tx - x) - yaw\n",
        "    head_err = (head_err + np.pi) % (2 * np.pi) - np.pi\n",
        "    dx = [x - icx for icx in course.cx]\n",
        "    dy = [y - icy for icy in course.cy]\n",
        "    lat_err = np.min(np.hypot(dx, dy))\n",
        "    sign = np.sign(np.cross([tx - x, ty - y], [np.cos(yaw), np.sin(yaw)]))\n",
        "    lat_err *= sign if sign != 0 else 1\n",
        "    return lat_err, head_err\n",
        "\n",
        "def train_qlearning(course, ql, num_episodes=50, steps_per_ep=600, dt=0.1, v=1.0, sigma_v=0.05, sigma_omega=0.02):\n",
        "    \"\"\"\n",
        "    Train Q-Learning controller on the racetrack.\n",
        "\n",
        "    Args:\n",
        "        course: TargetCourse object\n",
        "        ql: QLearningController object\n",
        "        num_episodes: Number of training episodes\n",
        "        steps_per_ep: Steps per episode\n",
        "        dt: Time step\n",
        "        v: Linear velocity\n",
        "        sigma_v, sigma_omega: Process noise std devs\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Training involves multiple episodes where the robot learns to adjust ω\n",
        "    to minimize lateral and heading errors. Each episode resets the pose,\n",
        "    and the agent explores actions while updating the Q-table based on rewards.\n",
        "    \"\"\"\n",
        "    for ep in range(num_episodes):\n",
        "        true_pose = np.array([0.0, 0.0, 0.0])  # Reset pose each episode\n",
        "        for t in range(steps_per_ep):\n",
        "            x, y, theta = true_pose\n",
        "            lat_err, head_err = get_errors(x, y, theta, course)\n",
        "            state = ql.discretize(lat_err, head_err)\n",
        "            action_idx = ql.choose_action(state, train=True)\n",
        "            omega_offset = ql.actions[action_idx]\n",
        "            delta, _ = pure_pursuit(x, y, theta, course, 0)\n",
        "            WB = 0.5\n",
        "            omega_base = v / WB * np.tan(delta)\n",
        "            omega = omega_base + omega_offset\n",
        "            v_true = v + np.random.normal(0, sigma_v)\n",
        "            omega_true = omega + np.random.normal(0, sigma_omega)\n",
        "            # Update true pose\n",
        "            if abs(omega_true) < 1e-6:\n",
        "                true_pose[0] += v_true * dt * np.cos(true_pose[2])\n",
        "                true_pose[1] += v_true * dt * np.sin(true_pose[2])\n",
        "                true_pose[2] += omega_true * dt\n",
        "            else:\n",
        "                true_pose[0] += (v_true / omega_true) * (np.sin(true_pose[2] + omega_true * dt) - np.sin(true_pose[2]))\n",
        "                true_pose[1] += (v_true / omega_true) * (-np.cos(true_pose[2] + omega_true * dt) + np.cos(true_pose[2]))\n",
        "                true_pose[2] += omega_true * dt\n",
        "            true_pose[2] = (true_pose[2] + np.pi) % (2 * np.pi) - np.pi\n",
        "            next_lat_err, next_head_err = get_errors(true_pose[0], true_pose[1], true_pose[2], course)\n",
        "            next_state = ql.discretize(next_lat_err, next_head_err)\n",
        "            reward = - (abs(next_lat_err) + 0.5 * abs(next_head_err))  # Negative error as reward\n",
        "            ql.update(state, action_idx, reward, next_state)\n",
        "\n",
        "def simulate_and_animate():\n",
        "    \"\"\"\n",
        "    Run full simulation with training and animation.\n",
        "\n",
        "    Returns:\n",
        "        ani: Animation object\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    The simulation consists of:\n",
        "    1. Training the Q-Learning controller.\n",
        "    2. Running the integrated SLAM with Q-Learning control for multiple laps.\n",
        "    3. Generating an animation of the SLAM process, showing trajectories and landmarks.\n",
        "    This demonstrates the combined performance of learning-based control and probabilistic mapping.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    cx, cy = generate_racetrack()\n",
        "    course = TargetCourse(cx, cy)\n",
        "    num_laps = 3\n",
        "    num_steps = 600 * num_laps\n",
        "    dt = 0.1\n",
        "    v = 1.0\n",
        "    max_range = 4.0\n",
        "    sigma_r = 0.1\n",
        "    sigma_b = 0.05\n",
        "    sigma_v = 0.05\n",
        "    sigma_omega = 0.02\n",
        "    num_true_lm = 15\n",
        "    landmarks = np.random.uniform(-5, 25, (num_true_lm, 2))  # Random landmarks around track\n",
        "    true_pose = np.array([0.0, 0.0, 0.0], dtype=float)\n",
        "    slam = EKFSLAM(robot_sigma_x=0.0, robot_sigma_y=0.0, robot_sigma_theta=0.0,\n",
        "                   measurement_sigma_r=sigma_r, measurement_sigma_b=sigma_b)\n",
        "    slam.mu = np.array([0.5, 0.2, 0.1], dtype=float)  # Initial estimate\n",
        "    slam.Sigma = np.diag([0.2**2, 0.2**2, 0.1**2])\n",
        "    ql = QLearningController()\n",
        "    train_qlearning(course, ql, num_episodes=50)  # Train Q-Learning\n",
        "    true_poses = [true_pose.copy()]\n",
        "    est_poses = [slam.mu[:3].copy()]\n",
        "    est_lms_hist = [slam.mu[3:].reshape(-1, 2).copy() if slam.lm_size > 0 else np.array([])]\n",
        "    cov_hist = [slam.Sigma.copy()]\n",
        "    target_ind = 0\n",
        "    \"\"\"\n",
        "    Main simulation loop: For each time step, compute errors, choose Q-Learning action,\n",
        "    predict SLAM state, simulate true motion with noise, generate measurements,\n",
        "    update SLAM, and record history for animation.\n",
        "    \"\"\"\n",
        "    for t in range(1, num_steps):\n",
        "        est_x, est_y, est_theta = slam.mu[0], slam.mu[1], slam.mu[2]\n",
        "        lat_err, theta_err = get_errors(est_x, est_y, est_theta, course)\n",
        "        state = ql.discretize(lat_err, theta_err)\n",
        "        action_idx = ql.choose_action(state, train=False)  # Greedy action\n",
        "        omega_offset = ql.actions[action_idx]\n",
        "        delta, target_ind = pure_pursuit(est_x, est_y, est_theta, course, target_ind)\n",
        "        WB = 0.5\n",
        "        omega_base = v / WB * np.tan(delta)\n",
        "        omega = omega_base + omega_offset\n",
        "        slam.prediction(v, omega, dt, sigma_v, sigma_omega)\n",
        "        v_true = v + np.random.normal(0, sigma_v)\n",
        "        omega_true = omega + np.random.normal(0, sigma_omega)\n",
        "        # Update true pose\n",
        "        if abs(omega_true) < 1e-6:\n",
        "            true_pose[0] += v_true * dt * np.cos(true_pose[2])\n",
        "            true_pose[1] += v_true * dt * np.sin(true_pose[2])\n",
        "            true_pose[2] += omega_true * dt\n",
        "        else:\n",
        "            true_pose[0] += (v_true / omega_true) * (np.sin(true_pose[2] + omega_true * dt) - np.sin(true_pose[2]))\n",
        "            true_pose[1] += (v_true / omega_true) * (-np.cos(true_pose[2] + omega_true * dt) + np.cos(true_pose[2]))\n",
        "            true_pose[2] += omega_true * dt\n",
        "        true_pose[2] = (true_pose[2] + np.pi) % (2 * np.pi) - np.pi\n",
        "        zs = []  # Measurements\n",
        "        for lm in landmarks:\n",
        "            dx = lm[0] - true_pose[0]\n",
        "            dy = lm[1] - true_pose[1]\n",
        "            dist = np.sqrt(dx**2 + dy**2)\n",
        "            if dist < max_range:\n",
        "                bearing = np.arctan2(dy, dx) - true_pose[2]\n",
        "                bearing = (bearing + np.pi) % (2 * np.pi) - np.pi\n",
        "                r_noisy = dist + np.random.normal(0, sigma_r)\n",
        "                b_noisy = bearing + np.random.normal(0, sigma_b)\n",
        "                zs.append([r_noisy, b_noisy])\n",
        "        slam.update(zs)\n",
        "        true_poses.append(true_pose.copy())\n",
        "        est_poses.append(slam.mu[:3].copy())\n",
        "        est_lms_hist.append(slam.mu[3:].reshape(-1, 2).copy() if slam.lm_size > 0 else np.array([]))\n",
        "        cov_hist.append(slam.Sigma.copy())\n",
        "    true_poses = np.array(true_poses)\n",
        "    est_poses = np.array(est_poses)\n",
        "    \"\"\"\n",
        "    Set up matplotlib figure and animation. The update function redraws the plot\n",
        "    for each frame, showing the evolution of trajectories, landmarks, and uncertainties.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.set_xlim(-5, 25)\n",
        "        ax.set_ylim(-5, 15)\n",
        "        ax.plot(true_poses[:frame, 0], true_poses[:frame, 1], 'g-', label='True Trajectory')\n",
        "        ax.plot(est_poses[:frame, 0], est_poses[:frame, 1], 'b-', label='Estimated Trajectory')\n",
        "        ax.scatter(landmarks[:, 0], landmarks[:, 1], c='k', marker='x', label='True Landmarks')\n",
        "        if est_lms_hist[frame].size > 0:\n",
        "            ax.scatter(est_lms_hist[frame][:, 0], est_lms_hist[frame][:, 1], c='r', marker='o', label='Est Landmarks')\n",
        "        plot_cov_ellipse(ax, est_poses[frame, :2], cov_hist[frame][:2, :2], color='blue')\n",
        "        for lm_idx in range(len(est_lms_hist[frame])):\n",
        "            j = 3 + 2 * lm_idx\n",
        "            pos = est_lms_hist[frame][lm_idx]\n",
        "            cov = cov_hist[frame][j:j+2, j:j+2]\n",
        "            plot_cov_ellipse(ax, pos, cov, color='red')\n",
        "        ax.plot(cx, cy, 'k--', label='Racetrack Path')\n",
        "        ax.legend()\n",
        "        ax.set_title(f'SLAM Convergence - Step {frame}')\n",
        "    ani = FuncAnimation(fig, update, frames=range(0, num_steps, 5), interval=50)\n",
        "    ani.save('slam.mp4', writer='ffmpeg')  # Save animation\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "# Run simulation and display animation\n",
        "ani = simulate_and_animate()\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niQoDeFasJtj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}